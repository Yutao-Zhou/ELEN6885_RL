{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62f79ae2-4e63-4ac2-adc7-f4d290ba3e95",
   "metadata": {
    "tags": []
   },
   "source": [
    "################################\n",
    "\n",
    "  ## PREP: install packages ##\n",
    "    \n",
    "################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeeb87af-a43d-4c28-952e-3ff4a7335ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda create --prefix=\"/local_path_to_save_env/\"  python=3.10\n",
    "!conda activate \"/local_path_to_save_env/\"\n",
    "!pip install trl\n",
    "!pip install transformer\n",
    "!pip install torch torchvision torchaudio\n",
    "!pip install peft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7e53c5-18ec-413f-84fa-0c6827ccce2d",
   "metadata": {},
   "source": [
    "################################\n",
    "\n",
    "  ## Load Modules ##\n",
    "    \n",
    "################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1f3ddc-9a24-4ac0-a487-7540c8bccda6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    RobertaForSequenceClassification,\n",
    "    RobertaTokenizer,\n",
    ")\n",
    "\n",
    "from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer, create_reference_model, set_seed\n",
    "from trl.core import LengthSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f10e757-25cc-4353-9d9e-7f942d50995b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef6c0f5-14ee-406b-a831-383fa8be9338",
   "metadata": {},
   "source": [
    "################################\n",
    "\n",
    "  ## 1. funcs for Data Preparation ##\n",
    "    \n",
    "################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b02af02-4a2e-42e7-a4a8-d0b6dd1dfade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_toxic_dataset(dataset_name: str, toxicity_threshold=0.3, cache_dir=None):\n",
    "    \"\"\"\n",
    "    Load a dataset from huggingface by using `load_dataset`\n",
    "    \n",
    "\n",
    "    Args:\n",
    "        dataset_name (`str`): a dataset to be loaded from huggingface.\n",
    "        toxicity_threshold (`float32`): threshold to determine if an input is toxic.\n",
    "        cache_dir (`str`): path to cache pretrained LLM.\n",
    "    \n",
    "    Returns:\n",
    "        a dataset\n",
    "    \"\"\"\n",
    "    if dataset_name == \"allenai/real-toxicity-prompts\":\n",
    "        ds = load_dataset(dataset_name, split=\"train\")\n",
    "\n",
    "        def select_toxic(sample):\n",
    "            toxicity = sample[\"prompt\"][\"toxicity\"]\n",
    "            return toxicity is not None and toxicity > toxicity_threshold\n",
    "    elif dataset_name == \"jigsaw_unintended_bias\":\n",
    "        dataset_name = \"./dataset/test_public_expanded.csv\"\n",
    "        ds = load_dataset(\"csv\", data_files=dataset_name)['train']\n",
    "        def select_toxic(sample):\n",
    "            toxicity = sample[\"toxicity\"]\n",
    "            return toxicity is not None and toxicity > toxicity_threshold\n",
    "    else:\n",
    "        raise ValueError(\"No such dataset used in the experiment.\")\n",
    "\n",
    "    ds = ds.filter(select_toxic, batched=False)\n",
    "    return ds\n",
    "\n",
    "def get_tokenized(tokenizer, dataset, dataset_name, min_text_length, max_text_length):\n",
    "    \"\"\"\n",
    "    Build dataset for training. This builds the dataset from `load_dataset`, one should\n",
    "    customize this function to train the model on its own dataset.\n",
    "\n",
    "    Args:\n",
    "        tokenizer : a huggingface tokenizer for text tokenization.\n",
    "        dataset (`dataset.Dataset`): A huggingface dataset to be loaded.\n",
    "        dataset_name (`str`): dataset name which helps to determine the way to tokenize.\n",
    "        min_text_length (`int`): minimal length of input.\n",
    "        max_text_length (`int`): maximal length of input.\n",
    "        \n",
    "    Returns:\n",
    "        dataloader (`torch.utils.data.DataLoader`): a dataloader for the dataset used in the training loop.\n",
    "    \"\"\"\n",
    "    \n",
    "    # sample by input length\n",
    "    input_size = LengthSampler(min_text_length, max_text_length)\n",
    "\n",
    "    def tokenize_real_toxicity_prompts(sample):\n",
    "        prompt = sample[\"prompt\"][\"text\"]\n",
    "        continuation = sample[\"continuation\"][\"text\"]\n",
    "\n",
    "        sample[\"input_ids\"] = tokenizer.encode(prompt + continuation)[: input_size()]\n",
    "        sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n",
    "        return sample\n",
    "    \n",
    "    def tokenize_jigsaw_unintended_bias(sample):\n",
    "        prompt = sample[\"comment_text\"]\n",
    "\n",
    "        sample[\"input_ids\"] = tokenizer.encode(prompt)[: input_size()]\n",
    "        sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n",
    "        return sample\n",
    "    \n",
    "    if dataset_name == \"allenai/real-toxicity-prompts\":\n",
    "        dataset = dataset.map(tokenize_real_toxicity_prompts, batched=False)\n",
    "    else:\n",
    "        dataset = dataset.map(tokenize_jigsaw_unintended_bias, batched=False)\n",
    "    \n",
    "    dataset.set_format(type=\"torch\")\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a46f05-1f36-4a35-86f2-e43c040ce2b9",
   "metadata": {},
   "source": [
    "########################################\n",
    "\n",
    "  ## 2. funcs for Load Model and Tokenizer ##\n",
    "    \n",
    "########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ed80b40-fde4-464d-9f45-7d42318e7a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_model_tokenizer(model_name_or_path, device=\"cpu\", cache_dir=None):\n",
    "    \"\"\"Loads a trained model from the given model name or path.\"\"\"\n",
    "    tokenizer = get_tokenizer(model_name_or_path, cache_dir=cache_dir)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        cache_dir=cache_dir,  # change to location you want to store the pretrained-model\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        # torch_dtype=torch.bfloat16 ## use torch.bfloat16 to save memory\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    return model, tokenizer\n",
    "def get_tokenizer(model_name_or_path, cache_dir):\n",
    "    if any(k in model_name_or_path for k in (\"gpt\", \"opt\", \"bloom\")):\n",
    "        padding_side = \"left\"\n",
    "    else:\n",
    "        padding_side = \"right\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, padding_side=padding_side, cache_dir=cache_dir)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004fa6ad-3bd9-4926-ac4a-8821794ab2e6",
   "metadata": {},
   "source": [
    "########################################\n",
    "\n",
    "  ## 3. Prepara data and model for experiments ##\n",
    "    \n",
    "########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47728274-0174-49f5-b543-60a3422688a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 8781/8781 [00:07<00:00, 1239.91 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# experiment config \n",
    "min_text_length = 20\n",
    "max_text_length = 40\n",
    "cache_dir = \"./cache\"\n",
    "model_name_or_path = \"EleutherAI/gpt-neo-125m\"\n",
    "device = \"cuda\"\n",
    "\n",
    "# load model and tokenizer\n",
    "model, tokenizer = load_pretrained_model_tokenizer(model_name_or_path, device=device, cache_dir=cache_dir)\n",
    "\n",
    "# load dataloader for dataset: \"allenai/real-toxicity-prompts\" or \"jigsaw_unintended_bias\"\n",
    "# dataset_name = \"allenai/real-toxicity-prompts\" # either \"jigsaw_unintended_bias\" or \"allenai/real-toxicity-prompts\"\n",
    "dataset_name = \"jigsaw_unintended_bias\"\n",
    "toxicity_threshold = 0.3\n",
    "test_ratio = 0.2\n",
    "dataset = load_toxic_dataset(dataset_name=dataset_name, toxicity_threshold=toxicity_threshold, cache_dir=cache_dir)\n",
    "dataset = dataset.train_test_split(test_size=test_ratio, shuffle=False)\n",
    "train_dataset = get_tokenized(tokenizer, dataset['train'], dataset_name, min_text_length, max_text_length)\n",
    "test_dataset = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b18c358-6706-416c-92a5-4fe8c82c6531",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'comment_text', 'created_date', 'publication_id', 'parent_id', 'article_id', 'rating', 'funny', 'wow', 'sad', 'likes', 'disagree', 'toxicity', 'severe_toxicity', 'obscene', 'sexual_explicit', 'identity_attack', 'insult', 'threat', 'identity_annotator_count', 'toxicity_annotator_count', 'male', 'female', 'transgender', 'other_gender', 'heterosexual', 'homosexual_gay_or_lesbian', 'bisexual', 'other_sexual_orientation', 'christian', 'jewish', 'muslim', 'hindu', 'buddhist', 'atheist', 'other_religion', 'black', 'white', 'asian', 'latino', 'other_race_or_ethnicity', 'physical_disability', 'intellectual_or_learning_disability', 'psychiatric_or_mental_illness', 'other_disability', 'input_ids', 'query'],\n",
       "    num_rows: 8781\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67eb45ec-0daa-4a6f-9a80-ca71adb75299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'comment_text', 'created_date', 'publication_id', 'parent_id', 'article_id', 'rating', 'funny', 'wow', 'sad', 'likes', 'disagree', 'toxicity', 'severe_toxicity', 'obscene', 'sexual_explicit', 'identity_attack', 'insult', 'threat', 'identity_annotator_count', 'toxicity_annotator_count', 'male', 'female', 'transgender', 'other_gender', 'heterosexual', 'homosexual_gay_or_lesbian', 'bisexual', 'other_sexual_orientation', 'christian', 'jewish', 'muslim', 'hindu', 'buddhist', 'atheist', 'other_religion', 'black', 'white', 'asian', 'latino', 'other_race_or_ethnicity', 'physical_disability', 'intellectual_or_learning_disability', 'psychiatric_or_mental_illness', 'other_disability'],\n",
       "    num_rows: 2196\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6399ffc-e855-4c5a-93ed-9c8c736f32bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "## 1. create reference model using traditional fine tuning\n",
    "model = AutoModelForCausalLMWithValueHead.from_pretrained(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a86e12b-cbbe-49bc-acd2-6ec99ed60606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of layers in total: 12\n"
     ]
    }
   ],
   "source": [
    "num_layers = len(model.pretrained_model.transformer.h)\n",
    "print(f\"number of layers in total: {num_layers}\")\n",
    "num_shared_layers = num_layers - 8 ## tuning on last two layers and freeze other layers\n",
    "reference_model = create_reference_model(model, num_shared_layers=num_shared_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b436f11-d52c-49d7-b94a-ae5526a8e271",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. create reference model using peft\n",
    "from peft import LoraConfig\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=128,\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLMWithValueHead.from_pretrained(model, peft_config=lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f89e642f-dfb5-4835-8121-6412d6704720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#trainable parameters: 56686849\n",
      "ratio of trainable parameters: 0.45277267030140833\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "total_params_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"#trainable parameters: {total_params_trainable}\")\n",
    "print(f\"ratio of trainable parameters: {total_params_trainable/total_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e70cb6-2229-4de5-9764-213b4a3d86fb",
   "metadata": {},
   "source": [
    "########################################\n",
    "\n",
    "  ## 4. Proximal Policy Optimizer ##\n",
    "    \n",
    "########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0bafc931-54d3-4f1a-bce9-217f13f115aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = (1.47e-5) * 2\n",
    "mini_batch_size = 64\n",
    "ppo_epochs = 100\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3db8ca5f-f314-468b-bde1-529ccdd98bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c04b9069-b2c0-4aa2-b087-54639b011636",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_config = PPOConfig(\n",
    "    model_name=model_name_or_path,\n",
    "    learning_rate=learning_rate,\n",
    "    ppo_epochs=ppo_epochs,\n",
    "    mini_batch_size=batch_size,\n",
    "    batch_size=batch_size,\n",
    "    gradient_accumulation_steps=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17bf291a-3e86-47bb-a18b-77d5ce7a8984",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "ppo_trainer = PPOTrainer(\n",
    "    ppo_config,\n",
    "    model,\n",
    "    ref_model=reference_model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=train_dataset,\n",
    "    data_collator=lambda data: dict((key, [d[key] for d in data]) for key in data[0]),\n",
    "    optimizer=optimizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d876752-56d8-44a9-8479-63f8879c232e",
   "metadata": {},
   "source": [
    "########################################\n",
    "\n",
    "  ## 5. Reward Model and Scoring ##\n",
    "  \n",
    "  #### use a pretrained classifier to score the toxicity of an input.\n",
    "    \n",
    "########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3136aeed-dab7-4988-8675-8ce1b4679231",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity_model_name_or_path = \"facebook/roberta-hate-speech-dynabench-r4-target\"\n",
    "toxicity_tokenizer = RobertaTokenizer.from_pretrained(toxicity_model_name_or_path, cache_dir=cache_dir)\n",
    "toxicity_model = RobertaForSequenceClassification.from_pretrained(toxicity_model_name_or_path, \n",
    "                                                                  cache_dir=cache_dir).to(ppo_trainer.accelerator.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492f68ee-a42a-4493-8f23-38f20ea4aa03",
   "metadata": {},
   "source": [
    "########################################\n",
    "\n",
    "  ## 6. Training the model to detoxify base model ##\n",
    "  \n",
    "    \n",
    "########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28de7535-9046-4394-a45e-7f2e69d99c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_new_tokens = 32\n",
    "generation_kwargs = {\n",
    "    \"min_length\": -1,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "    \"max_new_tokens\": max_new_tokens,\n",
    "}\n",
    "\n",
    "model_save_path = f\"./saved_model/{model_name_or_path}_detoxified_{dataset_name.split('/')[-1]}\"\n",
    "\n",
    "for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n",
    "    query_inputs = batch[\"input_ids\"]\n",
    "\n",
    "    # Get response from the policy model\n",
    "    outputs_tensors = []\n",
    "    for query in query_inputs:\n",
    "        response = ppo_trainer.generate(query, **generation_kwargs)\n",
    "        outputs_tensors.append(response.squeeze()[-max_new_tokens:])\n",
    "    batch[\"output\"] = [tokenizer.decode(r.squeeze()) for r in outputs_tensors]\n",
    "\n",
    "    # Compute toxicity score for each output\n",
    "    toxicity_inputs = toxicity_tokenizer(batch[\"output\"], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    toxicity_inputs = toxicity_inputs.to(ppo_trainer.accelerator.device)\n",
    "    logits = toxicity_model(**toxicity_inputs).logits.float()\n",
    "    toxicity_labels = (logits[:, 0]).tolist()\n",
    "\n",
    "    rewards = [torch.tensor(output) for output in toxicity_labels]\n",
    "\n",
    "    # Run PPO optimization step\n",
    "    stats = ppo_trainer.step(query_inputs, outputs_tensors, rewards)\n",
    "    ppo_trainer.log_stats(stats, batch, rewards)\n",
    "\n",
    "    # Save model every 20 epochs\n",
    "    if epoch and epoch % 20 == 0:\n",
    "        if ppo_trainer.accelerator.is_main_process:\n",
    "            ppo_trainer.save_pretrained(model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4040a9-2b59-4a18-b0c4-d1553503d87b",
   "metadata": {},
   "source": [
    "########################################\n",
    "\n",
    "  ## 7. Evaluate and Compare Results ##\n",
    "    \n",
    "########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "87cfcad2-4093-4a4c-b7fc-7204fa156d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import csv\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb86f669-5171-4399-adc9-319683ad557a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load toxicity from evaluation to score each model output\n",
    "toxicity = evaluate.load(\"ybelkada/toxicity\", \"DaNLP/da-electra-hatespeech-detection\", module_type=\"measurement\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "58c029bb-b3d6-4711-9cb0-035ef18aa59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES_TO_TEST = 500\n",
    "BATCH_SIZE = 64\n",
    "context_length = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6f8df0b2-6d3b-4d25-b2ef-93828ee04386",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union, Dict, Any\n",
    "from datasets import Dataset\n",
    "\n",
    "def tokenize_data(\n",
    "    tokenizer, input_data: Union[List[str], Dataset], config: Optional[Dict[str, Any]] = None):\n",
    "    \"\"\"Tokenize text data.\n",
    "\n",
    "    Args:\n",
    "        input_data: The text to be tokenized.\n",
    "        tokenizer: a tokenizer to tokenize.\n",
    "        config: parameters for setting up the tokenization. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        tokenized data Dict[str, Tensor]: tokenized data with input_ids, attention_masks and labels.\n",
    "    \"\"\"\n",
    "    if not isinstance(input_data, List):\n",
    "        input_data = input_data[\"text\"]\n",
    "\n",
    "    encoded: Dict[str, torch.Tensor] = tokenizer(\n",
    "        input_data,\n",
    "        padding=True,\n",
    "        # truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    return encoded\n",
    "\n",
    "def tokenize_on_dataset(tokenizer, dataset: Dataset, config: Optional[Dict[str, Any]] = None):\n",
    "    \"\"\"main function to perform tokenization over a dataset object\n",
    "\n",
    "    Args:\n",
    "        tokenizer (PreTrainedTokenizer): a tokenizer\n",
    "        dataset (Dataset): a dataset object to be tokenized, the feature *text* will be tokenized.\n",
    "        config (Dict, optional): parameters for setting up the tokenization. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        tokenized data Dict[str, Tensor]: tokenized data with input_ids, attention_masks and labels.\n",
    "    \"\"\"\n",
    "    tokenized_dataset = dataset.map(lambda x: tokenize_data(tokenizer, x, None), batched=True, num_proc=4)\n",
    "    return tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2d7d2715-8aa5-45fa-8f0b-118f8547c482",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_dist_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1569aee4-6156-46b2-937e-2d7a9df4a3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in-distribution test - coming from the same dataset that we used to train our detoxified model\n",
    "in_dist_ds = test_dataset\n",
    "def get_text(x):\n",
    "    if \"real\" in dataset_name:\n",
    "        x['text'] = x['prompt'][\"text\"][:context_length]\n",
    "    else:\n",
    "        x['text'] = x[\"comment_text\"][:context_length]\n",
    "    return x\n",
    "\n",
    "in_dist_ds = in_dist_ds.map(get_text, batched=False)\n",
    "in_dist_ds = tokenize_on_dataset(tokenizer, in_dist_ds)\n",
    "in_dist_ds = in_dist_ds.remove_columns([x for x in in_dist_ds.column_names if x not in ['input_ids', \"attention_mask\"]])\n",
    "in_dist_ds = in_dist_ds.select(range(NUM_SAMPLES_TO_TEST))\n",
    "in_dist_test_dataloader = DataLoader(\n",
    "        in_dist_ds, shuffle=False, collate_fn=default_data_collator, batch_size=BATCH_SIZE, pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5cfeeef8-44cb-4032-8c4d-bde98a6fc8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#out-distribution test - toxic data (not used in training our detoxified model)\n",
    "test_dataset_name = \"OxAISH-AL-LLM/wiki_toxic\"\n",
    "out_dist_ds = load_dataset(test_dataset_name, split=\"test\")\n",
    "out_dist_ds = out_dist_ds.filter(lambda x: x[\"label\"] == 1)\n",
    "out_dist_ds = out_dist_ds.rename_columns({\"comment_text\":\"text\"})\n",
    "def get_text2(x):\n",
    "    x['text'] = x[\"text\"][:context_length]\n",
    "    return x\n",
    "out_dist_ds = out_dist_ds.map(get_text2, batched=False)\n",
    "out_dist_ds = tokenize_on_dataset(tokenizer, out_dist_ds)\n",
    "out_dist_ds = out_dist_ds.remove_columns([x for x in out_dist_ds.column_names if x not in ['input_ids', \"attention_mask\"]])\n",
    "out_dist_ds = out_dist_ds.select(range(NUM_SAMPLES_TO_TEST))\n",
    "out_dist_test_dataloader = DataLoader(\n",
    "        out_dist_ds, shuffle=False, collate_fn=default_data_collator, batch_size=BATCH_SIZE, pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1eb54d81-7336-4be4-80d3-ed05a7fc5c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#out-distribution test - benign data\n",
    "benign_dataset_name = \"wikitext\"\n",
    "benign_ds = load_dataset(benign_dataset_name, \"wikitext-2-v1\", split=\"test\")\n",
    "def get_text2(x):\n",
    "    x['text'] = x[\"text\"][:context_length]\n",
    "    return x\n",
    "benign_ds = benign_ds.map(get_text2, batched=False)\n",
    "benign_ds = tokenize_on_dataset(tokenizer, benign_ds)\n",
    "benign_ds = benign_ds.remove_columns([x for x in benign_ds.column_names if x not in ['input_ids', \"attention_mask\"]])\n",
    "benign_ds = benign_ds.select(range(NUM_SAMPLES_TO_TEST))\n",
    "benign_test_dataloader = DataLoader(\n",
    "        benign_ds, shuffle=False, collate_fn=default_data_collator, batch_size=BATCH_SIZE, pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e514cac3-5d3f-4343-85b8-24a3dd2c07db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation setup\n",
    "models_to_be_tested = [\n",
    "    \"./saved_model/EleutherAI/gpt-neo-125m_detoxified_jigsaw_unintended_bias\",\n",
    "    # \"./saved_model/EleutherAI/gpt-neo-125m_detoxified\",\n",
    "    \"EleutherAI/gpt-neo-125m\",\n",
    "    # \"./saved_model/EleutherAI/gpt-neo-1.3b_detoxified\",\n",
    "    # \"EleutherAI/gpt-neo-1.3b\",\n",
    "]\n",
    "datasets_to_be_tested = {\n",
    "    dataset_name: in_dist_test_dataloader,\n",
    "    test_dataset_name: out_dist_test_dataloader,\n",
    "    benign_dataset_name: benign_test_dataloader\n",
    "}\n",
    "output_file = f\"./results/toxicity_{dataset_name.split('/')[-1]}.csv\"\n",
    "max_new_tokens = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ddc7607f-1e88-412d-a1bc-4a5d690f978b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]Some weights of the model checkpoint at ./saved_model/EleutherAI/gpt-neo-125m_detoxified_jigsaw_unintended_bias were not used when initializing GPTNeoForCausalLM: ['v_head.summary.weight', 'v_head.summary.bias']\n",
      "- This IS expected if you are initializing GPTNeoForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPTNeoForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/tmp/ipykernel_607227/3780600539.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs['input_ids'] = torch.tensor(inputs['input_ids']).to(device)\n",
      "/tmp/ipykernel_607227/3780600539.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs['attention_mask'] = torch.tensor(inputs['attention_mask']).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: ./saved_model/EleutherAI/gpt-neo-125m_detoxified_jigsaw_unintended_bias - Dataset: jigsaw_unintended_bias - Mean: 0.029199765555502383 - Std: 0.12521923485779027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: ./saved_model/EleutherAI/gpt-neo-125m_detoxified_jigsaw_unintended_bias - Dataset: OxAISH-AL-LLM/wiki_toxic - Mean: 0.029696088243625126 - Std: 0.10492667473797328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 50%|█████     | 1/2 [01:42<01:42, 102.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: ./saved_model/EleutherAI/gpt-neo-125m_detoxified_jigsaw_unintended_bias - Dataset: wikitext - Mean: 0.018494784566166347 - Std: 0.09972834850533008\n",
      "Model: EleutherAI/gpt-neo-125m - Dataset: jigsaw_unintended_bias - Mean: 0.3117448237745557 - Std: 0.3931699463907727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: EleutherAI/gpt-neo-125m - Dataset: OxAISH-AL-LLM/wiki_toxic - Mean: 0.3602497966245282 - Std: 0.4126089161176549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "100%|██████████| 2/2 [02:58<00:00, 89.30s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: EleutherAI/gpt-neo-125m - Dataset: wikitext - Mean: 0.10746023948129732 - Std: 0.2360730800055794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# open a csv file for writing results\n",
    "file = open(f\"{output_file}\", \"a\", newline=\"\")\n",
    "writer = csv.writer(file)\n",
    "# add first rows\n",
    "writer.writerow([\"model_id\", \"dataset_id\", \"mean_toxicity\", \"std_toxicity\"])\n",
    "\n",
    "for model_id in tqdm(models_to_be_tested):\n",
    "    model = None\n",
    "    torch.cuda.empty_cache()\n",
    "    toxicities = {}\n",
    "\n",
    "    if \"saved_model\" in model_id: # detoxified model\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id, \n",
    "                                                     device_map={\"\": device}, \n",
    "                                                    )\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    else: # base model (before detoxifying)\n",
    "        model, tokenizer = load_pretrained_model_tokenizer(model_id, device=device, cache_dir=cache_dir)\n",
    "\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    \n",
    "    for dataset_test in datasets_to_be_tested:\n",
    "        ds_data_loader = datasets_to_be_tested[dataset_test]\n",
    "        for inputs in ds_data_loader:\n",
    "            inputs['input_ids'] = torch.tensor(inputs['input_ids']).to(device)\n",
    "            inputs['attention_mask'] = torch.tensor(inputs['attention_mask']).to(device)\n",
    "            seq_length = inputs['input_ids'].size(1)\n",
    "            outputs = model.generate(**inputs, do_sample=True, max_new_tokens=max_new_tokens, use_cache=True)\n",
    "            generated_texts = tokenizer.batch_decode(outputs[:, seq_length:], skip_special_tokens=True)\n",
    "            toxicity_score = toxicity.compute(predictions=generated_texts)\n",
    "\n",
    "            if dataset_test not in toxicities:\n",
    "                toxicities[dataset_test] = []\n",
    "            toxicities[dataset_test].extend(toxicity_score[\"toxicity\"])\n",
    "\n",
    "        # compute mean & std using np\n",
    "        mean = np.mean(toxicities[dataset_test])\n",
    "        std = np.std(toxicities[dataset_test])\n",
    "\n",
    "        # save to file\n",
    "        writer.writerow([model_id, dataset_test, mean, std])\n",
    "\n",
    "        # print\n",
    "        print(f\"Model: {model_id} - Dataset: {dataset_test} - Mean: {mean} - Std: {std}\")\n",
    "\n",
    "# close file\n",
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
